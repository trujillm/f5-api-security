# F5 API Security Configuration Values - Direct Mode Only
# Copy this file to f5-ai-security-values.yaml and customize the values
# The f5-ai-security-values.yaml file is ignored by git to prevent accidental commits
# 
# This configuration focuses on Direct Mode RAG with external LLM integration

# ========================================================================
# DEPLOYMENT STRATEGY: Internal vs External LLMs
# ========================================================================
# 
# OPTION 1: External/Remote LLMs (RECOMMENDED)
# - Connect to existing RHOAI model serving endpoints
# - Use shared LLM services across multiple projects  
# - No model downloads, faster deployment, no GPU resources needed
# - Configure in the global.models section with url and apiToken
#
# OPTION 2: Internal LLMs (Resource Intensive)
# - Deploy and manage LLM models within this namespace
# - Requires GPU resources and model downloads (8GB+ per model)
# - Configure in the global.models section with enabled: true (no url)
# - Requires valid Hugging Face token for model downloads
#
# ========================================================================

# API Keys and Tokens
# Set your API keys and tokens here
# These will be used by the Helm chart during installation

# Hugging Face Token (required ONLY for internal model downloads)
# Get your token from: https://huggingface.co/settings/tokens
# Set this in the llm-service.secret.hf_token field below
# NOTE: Not needed if using external LLMs only

# TAVILY Search API Key is configured in the llama-stack.secrets section below

# LLM Service Configuration
llm-service:
  secret:
    # Hugging Face token - ONLY required for internal model deployments
    # Leave empty ("") if using external LLMs only
    hf_token: ""
    enabled: true

# ========================================================================
# MODEL CONFIGURATION
# ========================================================================
# Models are pre-defined in the llama-stack and llm-service subcharts.
# You only need to specify models here if you want to:
# 1. Enable existing models with custom settings
# 2. Add external/remote LLM endpoints
# 3. Override default model configurations
#
# Available pre-defined models (from ai-architecture-charts):
# - llama-3-2-1b-instruct
# - llama-3-1-8b-instruct  
# - llama-3-2-3b-instruct
# - llama-3-3-70b-instruct
# - llama-3-2-1b-instruct-quantized
# - llama-guard-3-1b
# - llama-guard-3-8b
# - qwen-2-5-vl-3b-instruct

global:
  models:
    # ====================================================================
    # OPTION 1: Enable Pre-defined Internal Models
    # ====================================================================
    # Uncomment and modify to enable internal model deployment:
    #
    # llama-3-1-8b-instruct:
    #   enabled: true
    #   tolerations:
    #     - key: "nvidia.com/gpu"
    #       operator: Exists
    #       effect: NoSchedule
    #
    # llama-guard-3-8b:
    #   enabled: true
    #   registerShield: true  # Register as safety shield
    #   tolerations:
    #     - key: "nvidia.com/gpu"
    #       operator: Exists
    #       effect: NoSchedule

    # ====================================================================
    # OPTION 2: External/Remote LLM Endpoints (RECOMMENDED)
    # ====================================================================
    # Add external LLM services to avoid model downloads and GPU usage:
    
           # Example 1: RHOAI Model Serving Endpoint
           # rhoai-llama-3-1-8b:
           #   id: redhataillama-31-8b-instruct  # Use exact model ID from endpoint
           #   url: https://redhataillama-31-8b-instruct-quickstart-llms.apps.ai-dev02.kni.syseng.devcluster.openshift.com/v1
           #   apiToken: "sk-dummy-key-for-public-endpoint"
           #   enabled: true

    # Example 2: OpenShift AI Shared Service
    # shared-llm-service:
    #   id: openai/meta-llama/Llama-3.1-8B-Instruct  # LiteLLM format
    #   url: https://shared-llm.shared-namespace.svc.cluster.local/v1
    #   apiToken: "your-service-token"
    #   enabled: true

    # Example 3: External OpenAI-Compatible Service
    # external-llm:
    #   id: custom-model-id
    #   url: https://external-llm-service.com/v1
    #   apiToken: "your-api-token"
    #   enabled: true

    # Example 4: Safety Model (External)
    # external-safety:
    #   id: meta-llama/Llama-Guard-3-8B
    #   url: https://safety-endpoint.com/v1
    #   apiToken: "safety-token"
    #   enabled: true
    #   registerShield: true  # Register as safety shield

    # Start with empty models - users add only what they need
    {}

  # ========================================================================
  # MCP SERVERS CONFIGURATION (Advanced)
  # ========================================================================
  # Model Context Protocol servers for additional tool integrations
  mcp-servers: {}

# Database Configuration (pgvector)
pgvector:
  secret:
    user: "postgres"
    password: "rag_password"
    dbname: "rag_blueprint"
    host: "pgvector"
    port: "5432"

# Llama Stack Configuration
llama-stack:
  secrets:
    # TAVILY Search API Key for web search functionality
    TAVILY_SEARCH_API_KEY: "Paste-your-key-here"
    
    # Add other API keys as needed
    # OPENAI_API_KEY: "your_openai_key_here"
    # ANTHROPIC_API_KEY: "your_anthropic_key_here"
 

# Default Chat Endpoint Configuration
chatEndpoint:
  # LlamaStack endpoint URL for chat completions
  url: "http://llamastack:8321"  # Default to local LlamaStack (change to your external endpoint)
  # Model ID to use for chat completions
  modelId: "rhoai-llama-3-1-8b/redhataillama-31-8b-instruct"
  # API key for endpoints that require authentication (leave empty for public endpoints)
  apiKey: ""  # Examples:
              # "sk-your-openai-api-key-here" for OpenAI
              # "your-anthropic-api-key-here" for Anthropic
              # "" for RHOAI/public endpoints (no key needed)

env:
  - name: "LLAMA_STACK_ENDPOINT"
    value: "http://llamastack:8321"
